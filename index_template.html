<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="stylesheet" type="text/css" href="index.css">
    <title>Naturally Occurring Equivariance in Neural Networks</title>
    <style>
      @media screen and (max-width: 1600px) {{
        .large-screen-only {{display: none;}}
      }}
      d-article hr {{
        margin-top: 70px !important;
        margin-bottom: 80px !important;
      }}
      .comma-list a {{
        margin-left: 6px;
        white-space: nowrap;
      }}
      d-contents {{
        max-width: 250px;
        grid-row: auto / span 3;
      }}
    </style>
  </head>

  <body>
    <d-front-matter>
      <code style="display: none;" type="text/json"
        >{{ 
        "title": "Naturally Occurring Equivariance in Neural Networks", 
        "description": "Neural networks naturally learn many transformed copies of the same feature, connected by symmetric weights.",
        "authors": [
          {{ "author": "Chris Olah", "authorURL": "https://colah.github.io", "affiliation": "OpenAI", "affiliationURL": "https://openai.com"  }},
          {{ "author": "Nick Cammarata", "authorURL": "http://nickcammarata.com", "affiliation": "OpenAI", "affiliationURL": "https://openai.com"  }},
          {{ "author": "Chelsea Voss", "authorURL": "", "affiliation": "OpenAI", "affiliationURL": "https://openai.com"  }},
          {{ "author": "Ludwig Schubert", "authorURL": "https://schubert.io/", "affiliation": "", "affiliationURL": ""  }},
          {{ "author": "Gabriel Goh", "authorURL": "https://gabgoh.github.io", "affiliation": "OpenAI", "affiliationURL": "https://openai.com"  }}
        ],
        "katex": {{
          "delimiters": [
          {{
            "left": "$$",
            "right": "$$",
            "display": true
          }},
            {{
              "left": "$",
              "right": "$",
              "display": false
            }}
          ]
        }}
       }}</code>
    </d-front-matter>
    <d-title>
      <h1>Naturally Occurring Equivariance in Neural Networks</h1>
    </d-title>
    <d-byline></d-byline>
    <d-article>


      <section id="thread-nav" class="thread-info" style="margin-top: 10px; margin-bottom: 40px;">
        <img class="icon" src="images/multiple-pages.svg" width="43px" height="50px">
        <p class="explanation">
            This article is part of the <a href="/2020/circuits/">Circuits thread</a>, an experimental format collecting invited short articles and critical commentary delving into the inner workings of neural networks.
        </p>
        <a class="prev" href="/2020/circuits/curve-detectors/">Curve Detectors</a>
        <a class="next" href="/2020/circuits/frequency-edges/">High-Low Frequency Detectors</a>
    </section>
      
     
    <d-contents>
      <nav class="l-text toc figcaption">
        <h3>Contents</h3>
        <div><a href="#features">Equivariant Features</a></div>
        <ul><li class='comma-list'>
          <a href="#rotation-features">Rotation</a>, 
          <a href="#scale-features">Scale</a>, 
          <a href="#hue-features">Hue</a>, 
          <a href="#hue-rotation-features">Hue-Rotation</a>, 
          <a href="#reflection-features">Reflection</a>, 
          <a href="#miscellaneous-features">Miscellaneous</a>
        </li></ul>
        <div><a href="#circuits"></a>Equivariant Circuits</div>
        <ul><li class='comma-list'>
          <a href="#circuit-hilo">High-Low</a>, 
          <a href="#circuit-greenpurple">Contrast→Center</a>, 
          <a href="#circuit-BW">BW-Color</a>, 
          <a href="#circuit-circlestar">Line→Circle/Divergence</a>, 
          <a href="#circuit-circle-evolute">Curve→Circle/Evolute</a>, 
          <a href="#circuit-person">Human-Animal</a>,
          <a href="#circuit-dog">Invariant Dog Head</a>, 
          <a href="#circuit-hue">Hue→Hue</a>,
          <a href="#circuit-curve">Curve→Curve</a>, 
          <a href="#circuit-coloredge">Contrast→Line</a>

          
        </li></ul>
        <div><a href="#architectures">Equivariant Architectures</a></div>
        <div><a href="#conclusion">Conclusion</a></div>
        
      </nav>
    </d-contents>








    <p>Convolutional neural networks contain a hidden world of symmetries within themselves. This symmetry is a powerful tool in understanding the features and circuits inside neural networks. It also suggests that efforts to design neural networks with additional symmetries baked in (eg. <d-cite key="bergstra2011statistical,cohen2016group,dieleman2016exploiting,cohen2016steerable,thomas2018tensor,winkels20183d"></d-cite>) may be on a promising track.

      <p>To see these symmetries, we need to look at the individual neurons inside convolutional neural networks and the <a href="https://distill.pub/2020/circuits/zoom-in/#claim-2">circuits</a> that connect them. 
      It turns out that many neurons are slightly transformed versions of the same basic feature.
      This includes rotated copies of the same feature, scaled copies, flipped copies, features detecting different colors, and much more.
      We sometimes call this phenomenon “<a href="https://en.wikipedia.org/wiki/Equivariant_map">equivariance</a>,” since it means that switching the neurons is equivalent to transforming the input.
          <d-footnote>The standard definition of <a href="https://en.wikipedia.org/wiki/Equivariant_map">equivariance</a> in group theory is that a function <d-math>f</d-math> is equivariant if for all <d-math>g\in G</d-math>, it's the case that <d-math>f(g\cdot x) = g\cdot f(x)</d-math>. At first blush, this doesn't seem very relevant to transformed versions of neurons.<br><br> Before we talk about the examples introduced in this article, let's talk about how this definition maps to the classic example of equivariance in neural networks: translation and convolutional neural network nets. In a conv net, translating the input image is equivalent to translating the neurons in the hidden layers (ignoring pooling, striding, etc). Formally, <d-math>g\in Z^2</d-math> and <d-math>f</d-math> maps images to hidden layer activations. Then <d-math>g</d-math> acts on the input image <d-math>x</d-math> by translating spatially, and acts on the activations by also spatially translating them.<br><br> Now let's consider the case of curve detectors (the first example in the Equivariant Features section), which have ten rotated copies. In this case, <d-math>g\in Z_{{10}}</d-math> and <d-math>f(x) = (\mathrm{{curve}}_1(x), ...)</d-math> maps a position at an image to a ten dimensional vector describing how much each curve detector fires. Then <d-math>g</d-math> acts on the input image <d-math>x</d-math> by rotating it around that position and <d-math>g</d-math> acts on the hidden layers by reorganizing the neurons so that their orientations correspond to the appropriate rotations. This satisfies, at least approximately, the original definition of equivariance.<br><br> This transformed neuron form of equivariance is a special case of equivariance. There are many ways a neural network could be equivariant without having transformed versions of neurons. Conversely, we'll also see a number of examples of equivariance that don't map exactly to the group theory definition of equivariance: some have "holes" where a transformed neuron is missing, while others consist of a set of transformations that have a weaker structure than a group or don't correspond to a simple action on the image. But this general structure remains.</d-footnote> 
      Equivariance can be seen as a kind of "<a href="https://distill.pub/2020/circuits/zoom-in/#claim-2-motifs">circuit motif</a>," analogous to motifs in systems biology <d-cite key="alon2019introduction"></d-cite>.

      <p>
      In this article, we’ll focus on examples in InceptionV1 <d-cite key="szegedy2015going"></d-cite> trained on ImageNet<d-cite key="deng2009imagenet"></d-cite>, but we’ve observed at least some equivariance in every model trained on natural images we’ve studied.
      
        <br><br><hr><br><br>
  
      <h2 id='features'>Equivariant Features</h2>
  
      <p><b id='rotation-features'>Rotational Equivariance:</b> One example of equivariance is rotated versions of the same feature. These are especially common in <a href="https://distill.pub/2020/circuits/early-vision/">early vision</a>, for example <a href="https://distill.pub/2020/circuits/curve-detectors/">curve detectors</a>, <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3a_high_low_frequency"> high-low frequency detectors</a>, and <a href="https://distill.pub/2020/circuits/early-vision/#group_conv2d2_line">line detectors</a>.
      
      <figure class="rot-features" style='max-width: 800px;'>{images/rot-features}</figure>
      
      <p>One can test that these are genuinely rotated versions of the same feature by taking examples that cause one to fire, rotating them, and checking that the others fire as expected. The <a href="https://distill.pub/2020/circuits/curve-detectors/">article</a> on curve detectors tests their equivariance through several experiments, including rotating stimuli that activate one neuron and seeing how the others respond.
      
  
      <div class='large-screen-only' style="grid-column: middle-end / screen-end; grid-row: auto / span 2; max-width: 250px;">
        <a href="https://distill.pub/2020/circuits/curve-detectors/#joint-tuning-curves" style='border-bottom: none;'>
        <img src="images/curve-rotate-small.png" style="max-width: 250px; filter: grayscale(30%) opacity(90%) ;" />
        </a>
        <figcaption>One way to verify that units like curve detectors are truly rotated versions of the same feature is to take stimuli that activate one and see how they fire as you rotate the stimuli. <a href="https://distill.pub/2020/circuits/curve-detectors/#joint-tuning-curves">Learn more.</a></figcaption>
      </div>
      
      <p><b id='scale-features'>Scale Equivariance:</b> Rotated versions aren’t the only kind of variation we see. It’s also quite common to see the same feature at different scales, although usually the scaled features occur at different layers. For example, we see <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_circles_loops">circle detectors</a> across a huge variety of scales, with the small ones in early layers and the large ones in later layers.
      
        <figure class="scale-features" style='max-width: 800px;'>{images/scale-features}</figure>
      
      <p><b id='hue-features'>Hue Equivariance:</b> For color-detecting features, we often see variants detecting the same thing in different hues. For example, <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3a_color_center_surround">color center-surround</a> units will detect one hue in the center, and the opposing hue on around it. Units can be found doing this up until the seventh or even eighth layer of InceptionV1.
      
        <figure class="hue-features" style='max-width: 800px;'>{images/hue-features}</figure>
      
      <p><b id='hue-rotation-features'>Hue-Rotation Equivariance:</b> In early vision, we very often see <a href="https://distill.pub/2020/circuits/early-vision/#group_conv2d2_color_contrast">color contrast units</a>. These units detect one hue on one side, and the opposite hue on the other. As a result, they have variation in both hue and rotation.  These variations are particularly interesting, because there’s an interaction between hue and rotation. But cycling hue by 180 degrees flips which hue is on which side, and is so is equivalent to rotating by 180 degrees.
      
      <p>In the following diagram, we show orientation rotating the whole 360 degrees, but hue only rotating 180. At the bottom of the chart, it wraps around to the top but shifts by 180 degrees.
      
      
      <figure class="huerot-features" style='max-width: 800px;'>{images/huerot-features}</figure>
      
      <p><b id='reflection-features'>Reflection Equivariance:</b> As we move into the mid layers of the network, rotated variations become less prominent, but horizontally flipped pairs become quite prevalent.
      
      <figure class="flip-features" style='max-width: 800px;'>{images/flip-features}</figure>
      
      <p><b id='miscellaneous-features'>Miscellaneous Equivariance:</b> Finally, we see variations of features transformed in other miscellaneous ways. For example, short vs long-snouted versions of the same dog head features, or human vs dog versions of the same feature. We even see units which are equivariant to camera perspective (found in a Places365<d-cite key="zhou2016places"></d-cite> model). These aren’t necessarily something that we would classically think of as forms of equivariance, but do seem to essentially be the same thing.
      
      <figure class="exotic-features" style='max-width: 800px;'>{images/exotic-features}</figure>
      
  
      <br><br><hr><br><br>
  
      
      <h2 id='circuits'>Equivariant Circuits</h2>
      
      <p>The equivariant behavior we observe in neurons is really a reflection of a deeper symmetry that exists in the weights of neural networks and the <a href="https://distill.pub/2020/circuits/zoom-in/#claim-2">circuits</a> they form.
      
      <p>We’ll start by focusing on rotationally equivariant features that are formed from rotationally invariant features. This “invariant→equivariant” case is probably the simplest form of equivariant circuit. Next, we'll look at “equivariant→invariant” circuits, and then finally the more complex “equivariant→equivariant” circuits.
  
      <br><br>
      
      <p><b id="circuit-hilo">High-Low Circuit:</b> In the following example, we see <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3a_high_low_frequency">high-low frequency detectors</a> get built from a high-frequency factor and a low-frequency factor (both factors correspond to a combination of neurons in the previous layer). Each high-low frequency detector responds to a transition in frequency in a given direction, detecting high-frequency patterns on one side, and low frequency patterns on the other. Notice how the same weight pattern rotates, making rotated versions of the feature.
      
      
      <figure class="circuit-diagram hilo-circuit" style='max-width: 800px;'>{images/hilo-circuit}</figure>
      
      <br><br>
      
      <p><b id="circuit-greenpurple">Contrast→Center Circuit:</b> This same pattern can be used in reverse to turn rotationally equivariant features back into rotationally invariant features (an “equivariant→invariant” circuit). In the following example, we see several green-purple <a href="https://distill.pub/2020/circuits/early-vision/#group_conv2d2_color_contrast">color contrast detectors</a> get combined to create green-purple and purple-green <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3a_color_center_surround">center-surround detectors</a>.
      
      Compare the weights in this circuit to the ones in the previous one. It’s literally the same weight pattern transposed.
      
      
      <figure class="circuit-diagram greenpurple-circuit" style='max-width: 800px;'>{images/greenpurple-circuit}</figure>
      
      
      <p>Sometimes we see one of these immediately follow the other: equivariance be created, and then immediately partially used to create invariant units.
      
      <br><br>
  
      <p><b id="circuit-BW">BW-Color Circuit:</b> In the following example, a generic color factor and a black and white factor are used to create black and white vs color features. Later, the <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3a_bw_vs_color">black and white vs color features</a> are combined to create units which detect black and white at the center, but color around, or vice versa.
      
      
      <figure class="circuit-diagram BW-circuit" style='max-width: 800px;'>{images/BW-circuit}</figure>
      
      <br><br>
      
      <p><b id="circuit-circlestar">Line→Circle/Divergence Circuit:</b> Another example of equivariant features being combined to create invariant features is very early line-like <a href="https://distill.pub/2020/circuits/early-vision/#group_conv2d1_complex_gabor">complex Gabor detectors</a> being combined to create a small circle unit and diverging lines unit.
      
      
      <figure class="circuit-diagram circlestar-circuit" style='max-width: 800px;'>{images/circlestar-circuit}</figure>
      
      <br><br>
  
      <p><b id="circuit-circle-evolute">Curve→Circle/Evolute Circuit:</b> For a more complex example of rotational equivariance being combined to create invariant units, we can look at <a href="https://distill.pub/2020/circuits/curve-detectors/">curve detectors</a> being combined to create <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_circles_loops">circle</a> and <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_evolute">evolute detectors</a>. This circuit is also an example of scale equivariance. The same general pattern which turns small curve detectors into a small circle detector turns large curve detectors into a large circle detector. The same pattern which turns medium curve detectors into a medium evolute detector turns large curves into a large evolute detector.
      
      <figure class="circuit-diagram circle-evolute-circuits" style='max-width: 800px;'>{images/circle-evolute-circuits}</figure>
      
      <br><br>
  
      <p><b id="circuit-person">Human-Animal Circuit:</b> So far, all of the examples we’ve seen of circuits have involved rotation. These human-animal and animal-human detectors are an example of horizontal flip equivariance instead:
      
      <figure class="circuit-diagram person-circuit" style='max-width: 800px;'>{images/person-circuit}</figure>
      
      <p><b id="circuit-dog">Invariant Dog Head Circuit:</b> Conversely, this example (part of the broader <a href="https://distill.pub/2020/circuits/zoom-in/#claim-2-dog">oriented dog head circuit</a>) shows left and right oriented dog heads get combined into a pose invariant dog head detector. Notice how the weights flip.
      
      <figure class="circuit-diagram dog-circuit" style='max-width: 400px;'>{images/dog-circuit}</figure>
      
      <h3>“Equivariant→Equivariant” Circuits</h3>
      
      <p>The circuits we’ve looked at so far were either “invariant→equivariant” or “equivariant→invariant.” Either they had invariant input units, or invariant output units. Circuits of this form are quite simple: the weights rotate, or flip, or otherwise transform, but only in response to the transformation of a single feature. When we look at “equivariant→equivariant” circuits, things become a bit more complex. Both the input and output features transform, and we need to consider the relative relationship between the two units.
  
      <br><br>
      
      <p><b id="circuit-hue">Hue→Hue Circuit:</b> Let’s start with a circuit connecting two sets of hue-equivariant center-surround detectors. Each unit in the second layer is excited by the unit selecting for a similar hue in the previous layer.
      
      
      <figure class="circuit-diagram hue-circuit" style='max-width: 800px;'>{images/hue-circuit}</figure>
      
      
      <p>To understand the above, we need to focus on the relative relationships between each input and output neuron — in this case, how far the hues are apart on the color wheel. When they have the same hue, the relationship is excitatory. When they have close but different hues, it's inhibitory. And when they are very different, the weight is close to zero. <d-footnote>The units used to illustrate hue equivariance here were selected to have a straightforward circuit. Other units may have more complex relationships. For example, some units respond to a range of hues like yellow-red and have correspondingly more complex weights.</d-footnote>
      
      <br><br>
  
      <p><b id="circuit-curve">Curve→Curve Circuit:</b> Let’s now consider a slightly more complex example, how early curve detectors connect to late curve detectors. We’ll focus on four <a href="https://distill.pub/2020/circuits/curve-detectors/">curve detectors</a> that are 90 degrees rotated from each other.<d-footnote>Again, the curve detectors presented were selected to make the circuit as simple and pedagogical as possible. They have clean weights and even spacing between them, which will make the pattern easier to see. A forthcoming article will discuss curve circuits in detail.</d-footnote>
      
      <p>If we just look at the matrix of weights, it’s a bit hard to understand. But if we focus on how each curve detector connects to the earlier curve in the same and opposite orientations, it becomes easier to see the structure. Rather than each curve being built from the same neurons in the previous layer, they shift. Each curve is excited by curves in the same orientation and inhibited by those in the opposite. At the same time, the spatial structure of the weights also rotate.
      
      
      <figure class="circuit-diagram curve-circuit" style='max-width: 800px;'>{images/curve-circuit}</figure>
      
      <br><br>
      
      <p><b id="circuit-coloredge">Contrast→Line Circuit:</b> For a yet more complex example, let’s look at how <a href="https://distill.pub/2020/circuits/early-vision/#group_conv2d2_color_contrast">color contrast detectors</a> connect to <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3a_lines">line detectors</a>. The general idea is line detectors should fire more strongly if there are different colors on each side of the line. Conversely, they should be inhibited by a change in color if it is perpendicular to the line.
      
      <p>Note that this is an “equivariant→equivariant” circuit with respect to rotation, but “equivariant→invariant” with respect to hue.
      
      <!--<figure class="circuit-diagram coloredge-circuit">images/coloredge-circuit</figure>-->
      
      <figure class="circuit-diagram coloredge-circuit" style='grid-column: text-start / page-end; max-width: 1000px;'>{images/coloredge-circuit-wide}</figure>
      
  
      <br><br><hr><br><br><br>
  
      <h2 id="architectures">Equivariant Architectures</h2>
  
      <p>
        Equivariance has a rich history in deep learning.
        Many important neural network architectures have equivariance at their core, and there is a very active thread of research around more aggressively incorporating equivariance.
        But the focus is normally on designing equivariant architectures, rather than "natural equivariance" we've discussed so far. 
        How should we think about the relationship between "natural" and "designed" equivariance?
        As we'll see, there appears to be quite a deep connection.
  
      <p>
        Historically, there has been some interesting back and forth between the two.
        Researchers have often observed that many features in the first layer of neural networks are transformed versions of one basic template.<d-footnote>Features in the first layer of neural networks are much more often studied than in other layers. This is because they are easy to study: you can just visualize the weights to pixel values, or more generally to input features.</d-footnote> 
        This naturally occurring equivariance in the first layer has then sometimes been — and in other cases, easily could have been — inspiration for the design of new architectures.
  
  
      
      <p>For example, if you train a fully-connected neural network on a visual task, the first layer will learn variants of the same features over and over: Gabor filters at different positions, orientations, and scales.  Convolutional neural networks changed this. By baking the existence of translated copies of each feature directly into the network architecture, they generally remove the need for the network to learn translated copies of each feature. This resulted in a massive increase in statistical efficiency, and became a cornerstone of modern deep learning approaches to computer vision. But if we look at the first layer of a well-trained convolutional neural network, we see that other transformed versions of the same feature remain:
      
      <figure style="max-width:700px;" >
        <img src="images/slim_conv1.png" />
        <figcaption>The weights for the units in the first layer of the TF-Slim <d-cite key="silberman2018tf"></d-cite>
           version of InceptionV1 <d-cite key="szegedy2015going"></d-cite>.<d-footnote>We show the first layer conv weights of the tf-slim version of InceptionV1 rather than the canonical one because its weights are cleaner. This is likely due to the inclusion of batch-norm in the slim variant, causing cleaner gradients.</d-footnote> Units are sorted by the first principal component of the adjacency matrix between the first and second layers. Note how many features are similar except for rotation, scale, and hue.</figcaption>
      </figure>
      
      
      <p>Inspired by this, a 2011 paper subtitled “One Gabor to Rule Them All” <d-cite key="bergstra2011statistical"></d-cite> created a sparse coding model which had a single Gabor filter translated, rotated, and scaled. In more recent years, a number of papers have extended this equivariance to the hidden layers of neural networks, and to broader kinds of transformations <d-cite key="cohen2016group,dieleman2016exploiting,cohen2016steerable,winkels20183d"></d-cite>. Just as convolutional neural networks enforce that the weights between two features be the same if they have the same relative position:
      
      
      $$W_{{(x_1,~y_1,~a) ~\to~ (x_2,~y_2,~b)}} ~~=~~ W_{{(x_1+\Delta x,~y_1 +\Delta y,~a) ~\to~ (x_2+\Delta x,~y_2+\Delta y,~b)}}$$
      
      <p>… these more sophisticated equivariant networks make the weights between two neurons equal if they have the same relative relationship under more general transformations:<d-footnote>
        For our purposes, it suffices to know that these equivariant neural networks have the same weights when there is the same relative relationship between neurons. This footnote is for the benefit of readers who may wish to engage more deeply in the enforced equivariance literature, and can be safely skipped.
        <br><br>
        Group theory is an area of mathematics that gives us tools for describing symmetries and sets of interacting transformations. To build equivariant neural networks, we often borrow an idea from group theory called a group convolution. Just as a regular convolution can describe weights that correctly respect translational equivariance, a group convolution can describe weights that respect a complex set of interacting transformations (the group it operates over). Although you could try to work out how to tie the weights to achieve this from first principles, it’s easy to make mistakes. (One of the authors participated in many conversations with researchers in 2012 where people made errors on whiteboards about how sets of rotated and translated features should interact, without using convolutions.) Group convolutions can take any group you describe and give you the correct weight tying.
        <br><br>
        For an approachable introduction to group convolutions, we recommend <a href="https://colah.github.io/posts/2014-12-Groups-Convolution/">this article</a>.
        <br><br>
        If you dig further, you may begin to see papers discussing something called a group representation instead of group convolutions. This is a more advanced topic in group theory. The core idea is analogous to the Fourier transform. Recall that the Fourier transform turns convolution into pointwise multiplication (this is sometimes used to accelerate convolution). Well, the Fourier transform has a version that can operate over functions on groups, and also maps convolution to pointwise multiplication. And when you apply the Fourier transform to a group, the resulting coefficients correspond to something called a group representation, which you can think of as being analogous to a frequency in the regular Fourier transform.
      </d-footnote>
      
      
      $$W_{{a~\to~ b}} ~~=~~ W_{{T(a) \to T(b)}}$$
      
      <p>This is, at least approximately, what we saw conv nets naturally doing when we look at equivariant circuits! The weights had symmetries that caused neurons with similar relationships to have similar weights, much like an equivariant architecture would force them to. 
      
      <p>Given that we have neural network architectures which mimic the natural structures we observe, it seems natural to wonder what features and circuits such models learn. Do they learn the same equivariant features we see naturally form? Or do they do something entirely different?
      To answer these questions, we trained an equivariant model roughly inspired by InceptionV1 on ImageNet. We made half the neurons rotationally equivariant<d-cite key="cohen2016group"></d-cite> (with 16 rotations), and made the others rotationally invariant. Since we put no effort into tuning it, the model achieved abysmal test accuracy but still learns interesting features.<d-footnote>
        Here are the full set of features learned by the equivariant model. Half are forced to be rotationally equivariant, while half are forced to be rotationally invariant.
        <br><br>
        <img src="images/equiv.png" style='width: 100%; max-width: 800px;'>
        </d-footnote>
      Looking at mixed3b, we found that the equivariant model learned analogues of many large rotationally equivariant families from InceptionV1, such as <a href="https://distill.pub/2020/circuits/curve-detectors/">curve detectors</a>, <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_boundary">boundary detectors</a>, <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_divots">divot detectors</a>, and <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_generic_oriented_fur">oriented fur detectors</a>:
        
        
        <!-- many of the neurons were our familiar friends all the same. On the right (“equivariant units”), we see several curve and shallow curve features, a divot detector, a boundary detector, and an oriented fur detector. (There are also several features which aren’t immediately familiar to us.) -->
      
      <figure class="equiv-compare" style='max-width: 800px;'>{images/equiv-compare}</figure>
      
  
      
      <p>
        The existence of analogous features in equivariant models can be seen as a successful prediction of interpretability.
        As researchers engaged in more qualitative research, we should always be worried that we may be fooling ourselves. 
        Successfully predicting which features will form in an equivariant neural network architecture is actually a pretty non-trivial prediction to make, and a nice confirmation that we’re correctly understanding things.
      
      <p>
        Another exciting possibility is that this kind of feature and circuit analysis may be able to help inform equivariance research.
        For example, the kinds of equivariance that naturally form might be helpful in informing what types of equivariance we should design into different layers of a neural network.
  
      <!-- <h3>When Should We Expect Equivariance to Help Performance?</h3>
      
      <p>Convolutional neural networks have been enormously successful by baking in translational equivariance. One might have hoped that similar gains would be achieved by adding other kinds of equivariance to neural network architectures. Unfortunately, this has generally not been true, at least in the domain of images. The most studied case is rotational equivariance. Although rotation has led to significant improvements in some special cases [], it has generally not led to significant improvements on natural vision tasks like ImageNet.
      
      <p>If neural networks want to be equivariant so much that they try to learn equivariant features, why has equivariance not helped more? Can natural equivariance shed light on when we should expect equivariance to actually improve performance?
      
      <p>Our basic theory for why equivariance doesn’t help more is that the fraction of the model that “wants” to be equivariant to the transformations people try is too small.
      
      <p>To make this concrete, it’s useful to think about how many parameters need to be learned to implement a behavior. Let’s try to imagine how many parameters a partially equivariant model would need to mimic a normal model which has learned some equivariance features. If it needs significantly less parameters, that suggests that equivariance might be helpful.
      
      <p>Specifically, let’s imagine a network architecture which enforces some kind of equivariance — say rotation — with $K$ copies of each feature, but only for the fraction of neurons which would naturally develop that form equivariance. The other neurons are left as regular, convolutional neurons. Let’s call the fraction of units which are equivariant $r$.
      
      <p>In our partially equivariant model, all the parameters that involve non-equivariant units will need to be learned normally. But the parameters connecting equivariant neurons to other equivariant neurons ($r^2$ of the parameters) are reduced by a factor $K$. This means the total reduction in parameters is a ratio of:
      
      
      
      $$\frac{{N_\text{{new params}}}}{{N_\text{{old params}}}} ~~=~~ 1 - \frac{{K-1}}{{K}}r^2$$
      
      <p>Note that no matter what K is, this can only reduce, it can only reduce the parameters by at most r^2, the number of parameters connecting equivariant features. And this is the best case! If you were to force equivariance on features for which it isn’t useful (say an upside down dog head detector), you’d increase the number of parameters needed for those features by K instead.
      
      <p>So what fraction of neurons in a model are rotationally equivariant? Well, rotational equivariance  (and rotational invariance, which can also be included) is quite common in early vision. Perhaps 80% of features in early vision fall into these categories. But early vision is only about 10% of InceptionV1 — most neurons are in late vision, where rotational equivariance is rare. Let’s say that overall, 10% of neurons are rotationally equivariant. Well, ten percent squared is only one percent of parameters, so we shouldn’t expect rotationaly equivariance to be able to reduce the parameter account by more than 1%.
      
      <p>This is a heuristic argument, and there’s all sorts of ways it could be wrong, but it offers a possible explanation of why equivariant architectures have had limited success. And it may suggest a way to improve: in later layers, we observe “exotic equivariance” where features are equivariant to things like human vs dog, long vs short dog snout, or pose. It’s not fully clear how widespread these exotic notions of equivariance are. They likely can’t be formalized in terms of group actions and convolutions. But if a framework for them could be developed, perhaps it could make further equivariance successful for natural vision tasks.
      
       -->
      
      <!-- <h3>When Should We Expect Equivariance to Help Interpretability?</h3>
      
  
       -->
  
       <hr>
  
       <h2 id="conclusion">Conclusion</h2>
  
       <p>Equivariance has a remarkable ability to simplify our understanding of neural networks. When we see neural networks as families of features, interacting in structured ways, understanding small templates can actually turn into understanding how large numbers of neurons interact. Equivariance is a big help whenever we discover it.
      
      <p>We sometimes think of understanding neural networks as being like reverse engineering a regular computer program. In this analogy, equivariance is like finding the same inlined function repeated throughout the code. Once you realize that you’re seeing many copies of the same function, you only need to understand it once.
      
      <p>But natural equivariance does have some limitations. For starters, we have to find the equivariant families. This can actually take us quite a bit of work, poring through neurons. Further, they may not be exactly equivariant: one unit may be wired up slightly differently, or have a small exception, and so understanding it as equivariant could leave gaps in our understanding.
      
      <p>We’re excited about the potential of equivariant architectures to make the features and circuits of neural networks easier to understand. This seems especially promising in the context of early vision, where the vast majority of features seem to be equivariant to rotation, hue, scale, or a combination of those.
      
      <p>One of the biggest — and least discussed — advantages we have over neuroscientists in studying vision in artificial neural networks instead of biological neural networks is translational equivariance. By only having one neuron for each feature instead of tens of thousands of translated copies, convolutional neural networks massively reduce the complexity of studying artificial vision systems relative to biological ones. This has been a key ingredient in making it at all plausible that we can systematically understand InceptionV1.
      
      <p>Perhaps in the future, the right equivariant architecture will be able to shave another order of magnitude of complexity off of understanding early vision in neural networks. If so, understanding early vision might move from “possible with effort” to “easily achievable.”
  
  
  
  




      <section id="thread-nav" class="thread-info">
          <img class="icon" src="images/multiple-pages.svg" width="43px" height="50px">
          <p class="explanation">
              This article is part of the Circuits thread, a collection of short articles and commentary by an open scientific collaboration delving into the inner workings of neural networks.<br>
          </p>

          <a class="prev" href="/2020/circuits/curve-detectors/">Curve Detectors</a>
          <a class="next" href="/2020/circuits/frequency-edges/">High-Low Frequency Detectors</a>
      </section>





    </d-article>


    <d-appendix>
      <h3>Author Contributions</h3>

      <p><b>Research:</b> Examples of equivariance emerged across many investigations of features and circuits, so it's hard to separate out contributions in originally discovering it. Chris curated examples of different kinds of equivariant features and circuits. Nick introduced the framing of equivariance being a "motif", similar to motifs in systems biology, and did a very in-depth exploration of it in the context of curve detectors. Chelsea and Ludwig also did a fairly in-depth investigation of equivariance in the context of high-low frequency detectors. Gabe contributed to early research in circuits which helped surface equivariance.</p>

      <p><b>Writing and Diagrams:</b> Chris wrote and illustrated this article, with feedback from other authors.</p>

      <h3>Acknowledgments</h3>
      
      <p>
        We are very grateful to Taco Cohen for his comments and encouragement on the relationship between circuits and equivariance. We're also very grateful to Vincent Tjeng and Daniel Filan who gave detailed remarks on drafts and pointed out several things that were poorly communicated in an earlier draft, and to Smitty van Bodegom who caught and debugged a subtle cross-browser compatibility issue. Additionally, we appreciate the comments and support of Shan Carter, Tess Smidt, David Valdman, Peter Whidden, Laura Gunsalus, Christian Ng, Yaakov Saxon, Sara Sabour, and Yen Ong.
      </p>

      <d-footnote-list></d-footnote-list>
      <d-citation-list></d-citation-list>
    </d-appendix>

    <d-bibliography src="bibliography.bib"></d-bibliography>

    <script src="https://distill.pub/template.v2.js"></script>

  </body>
</html>
